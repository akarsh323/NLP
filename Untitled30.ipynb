{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1fKSzHE6pSFfONLv+3ZM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akarsh323/NLP/blob/main/Untitled30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV15ajpNc3HD",
        "outputId": "ee5d3c77-6789-44d7-c30d-3f274cbd1eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: el gato es grande\n",
            "Translated: the cat is <END>\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenize text into words. This is a simple tokenizer that\n",
        "    splits on whitespace and removes punctuation.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "def train_translation_model(corpus):\n",
        "    \"\"\"\n",
        "    Train a simple word-level translation model.\n",
        "\n",
        "    :param corpus: List of tuples (source_sentence, target_sentence)\n",
        "    :return: Dictionary mapping source words to {target_word: count}\n",
        "    \"\"\"\n",
        "    translation_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for source, target in corpus:\n",
        "        source_words = tokenize(source)\n",
        "        target_words = tokenize(target)\n",
        "\n",
        "        # Simplifying assumption: words align one-to-one in order\n",
        "        for s, t in zip(source_words, target_words):\n",
        "            translation_counts[s][t] += 1\n",
        "\n",
        "    # Convert counts to probabilities\n",
        "    translation_probs = {}\n",
        "    for s_word, t_counts in translation_counts.items():\n",
        "        total = sum(t_counts.values())\n",
        "        translation_probs[s_word] = {t: count/total for t, count in t_counts.items()}\n",
        "\n",
        "    return translation_probs\n",
        "\n",
        "def train_language_model(sentences, n=2):\n",
        "    \"\"\"\n",
        "    Train a simple n-gram language model.\n",
        "\n",
        "    :param sentences: List of sentences in the target language\n",
        "    :param n: n-gram size (default: bigrams)\n",
        "    :return: Dictionary mapping n-grams to their probabilities\n",
        "    \"\"\"\n",
        "    n_gram_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = tokenize(sentence) + ['<END>']\n",
        "        for i in range(len(words)-n+1):\n",
        "            n_gram = tuple(words[i:i+n])\n",
        "            next_word = words[i+n] if i+n < len(words) else None\n",
        "            n_gram_counts[n_gram][next_word] += 1\n",
        "\n",
        "    # Convert counts to probabilities\n",
        "    n_gram_probs = {}\n",
        "    for gram, next_word_counts in n_gram_counts.items():\n",
        "        total = sum(next_word_counts.values())\n",
        "        n_gram_probs[gram] = {w: count/total for w, count in next_word_counts.items()}\n",
        "\n",
        "    return n_gram_probs\n",
        "\n",
        "def translate(text, translation_model, language_model):\n",
        "    \"\"\"\n",
        "    Translate a sentence using the trained models.\n",
        "\n",
        "    :param text: Source text to translate\n",
        "    :param translation_model: Word-level translation probabilities\n",
        "    :param language_model: N-gram language model probabilities\n",
        "    :return: Most probable translation\n",
        "    \"\"\"\n",
        "    words = tokenize(text)\n",
        "    best_translation = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in translation_model:\n",
        "            # Choose the most probable translation for this word\n",
        "            best_word = max(translation_model[word], key=translation_model[word].get)\n",
        "        else:\n",
        "            # If word is unknown, keep it as-is (could be a name or number)\n",
        "            best_word = word\n",
        "        best_translation.append(best_word)\n",
        "\n",
        "    # Simple n-gram reordering and word choice\n",
        "    # This is a very simplistic decoding and in real-world systems\n",
        "    # would be much more complex (e.g., beam search)\n",
        "    final_translation = []\n",
        "    i = 0\n",
        "    while i < len(best_translation):\n",
        "        current_gram = tuple(best_translation[max(0, i-1):i+1])\n",
        "        if current_gram in language_model:\n",
        "            next_word = max(language_model[current_gram], key=language_model[current_gram].get)\n",
        "            if next_word is not None:\n",
        "                final_translation.append(next_word)\n",
        "        else:\n",
        "            final_translation.append(best_translation[i])\n",
        "        i += 1\n",
        "\n",
        "    return ' '.join(final_translation)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Toy corpus for demonstration (Spanish to English)\n",
        "    corpus = [\n",
        "        (\"el gato negro\", \"the black cat\"),\n",
        "        (\"el perro es grande\", \"the dog is big\"),\n",
        "        (\"me gusta el gato\", \"i like the cat\"),\n",
        "        (\"el perro corre\", \"the dog runs\"),\n",
        "    ]\n",
        "\n",
        "    # Train models\n",
        "    translation_model = train_translation_model(corpus)\n",
        "    language_model = train_language_model([t for _, t in corpus])\n",
        "\n",
        "    # Translate a sentence\n",
        "    source_text = \"el gato es grande\"\n",
        "    translated_text = translate(source_text, translation_model, language_model)\n",
        "\n",
        "    print(f\"Original: {source_text}\")\n",
        "    print(f\"Translated: {translated_text}\")"
      ]
    }
  ]
}